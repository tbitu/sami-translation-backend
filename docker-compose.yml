services:
  sami-translation-backend:
    # Use the prebuilt GHCR image (same as README docker run command)
    image: ghcr.io/tbitu/sami-translation-backend:latest
    
    # Alternative: build from source (uncomment if developing locally)
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    #   args:
    #     BASE_IMAGE: ${BASE_IMAGE:-nvcr.io/nvidia/pytorch:25.12-py3}
    ports:
      - "8000:8000"
    environment:
      # Keep a single worker (also enforced in Dockerfile CMD)
      - MODEL_DTYPE=fp32
      # Optional: for private HF repos
      # - HF_TOKEN=${HF_TOKEN}
      # Cache is mounted at /data/hf (see volumes + Dockerfile ENV)
      - HF_CACHE_DIR=/data/hf
    volumes:
      # Default: use a named volume for a persistent cache store.
      - hf-cache:/data/hf

      # Alternative: bind-mount a host directory (useful if you want Docker and
      # non-Docker processes to share the same HF cache).
      #
      # IMPORTANT (rootless / devcontainer): Docker volume source paths are
      # evaluated by the *host* daemon. If you run `docker compose` from a
      # devcontainer (where $HOME may be /root), set HF_CACHE_HOST_DIR to a host
      # path (e.g. /home/<user>/.cache/huggingface).
      # - ${HF_CACHE_HOST_DIR:-${HOME}/.cache/huggingface}:/data/hf
    # GPU support:
    # - With Docker Engine + NVIDIA Container Toolkit, either:
    #   docker compose run --gpus all ...
    # - Or uncomment the section below (Compose v2+ typically supports this):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  hf-cache:
    name: hf-cache
